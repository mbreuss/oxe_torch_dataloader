{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2be0d1f",
   "metadata": {},
   "source": [
    "# Step 2: Run Inference on Full Trajectories\n",
    "\n",
    "That was easy! Now let's try to run inference across a whole trajectory and visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0f7fd1-5b43-480f-b00f-766248d7f9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcelr/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-08 20:10:10.257729: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 20:10:10.257816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 20:10:10.259238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 20:10:10.267650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 20:10:11.127851: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# !IMPORTANT!, else Hydra isnt working !\n",
    "current_path = os.getcwd()\n",
    "sys.path.insert(0, Path(current_path).absolute().parents[0].as_posix())\n",
    "\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np\n",
    "import torch\n",
    "from uha import make_pytorch_oxe_iterable_dataset, get_octo_dataset_tensorflow, get_single_dataset_tensorflow, get_sequential_dataset_tensorflow\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "import omegaconf\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Next, we will load a trajectory from the Bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead523e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = omegaconf.OmegaConf.load(\"../uha/data/conf/uha_default_load_config.yaml\")\n",
    "cfg_transforms = omegaconf.OmegaConf.load(\"../uha/data/conf/transforms/oxe_no_remapping.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392bd127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################\n",
      "constructing single val dataset: bridge_dataset\n",
      "########################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 20:11:22.400466: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "2024-07-08 20:11:22.738188: I tensorflow/core/grappler/optimizers/data/replicate_on_split.cc:32] Running replicate on split optimization\n",
      "W0000 00:00:1720462288.662468  645295 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462288.663231  645295 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n",
      "W0000 00:00:1720462289.726431  645246 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462289.728241  645246 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n",
      "W0000 00:00:1720462289.764436  645255 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462289.769607  645255 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n",
      "W0000 00:00:1720462289.885051  645261 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462289.886950  645261 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n",
      "W0000 00:00:1720462289.897273  645262 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462289.900593  645262 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n",
      "W0000 00:00:1720462290.860318  645300 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -6 } } } inputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -2 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -2 } dim { size: -13 } dim { size: -14 } dim { size: -6 } } }\n",
      "W0000 00:00:1720462290.870018  645300 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"CropAndResize\" attr { key: \"T\" value { type: DT_FLOAT } } attr { key: \"extrapolation_value\" value { f: 0 } } attr { key: \"method\" value { s: \"bilinear\" } } inputs { dtype: DT_FLOAT shape { dim { size: 1 } dim { size: 256 } dim { size: 256 } dim { size: -8 } } } inputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: 4 } } } inputs { dtype: DT_INT32 shape { dim { size: -4 } } } inputs { dtype: DT_INT32 shape { dim { size: 2 } } } device { type: \"CPU\" vendor: \"AuthenticAMD\" model: \"241\" frequency: 2395 num_cores: 128 environment { key: \"cpu_instruction_set\" value: \"AVX SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2\" } environment { key: \"eigen\" value: \"3.4.90\" } l1_cache_size: 32768 l2_cache_size: 524288 l3_cache_size: 268435456 memory_size: 268435456 } outputs { dtype: DT_FLOAT shape { dim { size: -4 } dim { size: -18 } dim { size: -19 } dim { size: -8 } } }\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image dimensions (1, 256, 256, 3) are invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[1;32m     18\u001b[0m   images\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_primary\u001b[39m\u001b[38;5;124m\"\u001b[39m][i, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()) \u001b[38;5;66;03m# [batch_size, window_size, rgb, width, height]\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmediapy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# mediapy.show_video(images_wrist, fps=10)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/mediapy/__init__.py:1862\u001b[0m, in \u001b[0;36mshow_video\u001b[0;34m(images, title, **kwargs)\u001b[0m\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow_video\u001b[39m(\n\u001b[1;32m   1840\u001b[0m     images: Iterable[_NDArray], \u001b[38;5;241m*\u001b[39m, title: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m   1841\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Displays a video in the IPython notebook and optionally saves it to a file.\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \n\u001b[1;32m   1844\u001b[0m \u001b[38;5;124;03m  See `show_videos`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1860\u001b[0m \u001b[38;5;124;03m    html string if `return_html` is `True`.\u001b[39;00m\n\u001b[1;32m   1861\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshow_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/mediapy/__init__.py:1944\u001b[0m, in \u001b[0;36mshow_videos\u001b[0;34m(videos, titles, width, height, downsample, columns, fps, bps, qp, codec, ylabel, html_class, return_html, **kwargs)\u001b[0m\n\u001b[1;32m   1942\u001b[0m   video \u001b[38;5;241m=\u001b[39m [resize_image(image, (h, w)) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m video]\n\u001b[1;32m   1943\u001b[0m   first_image \u001b[38;5;241m=\u001b[39m video[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1944\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcompress_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodec\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _config\u001b[38;5;241m.\u001b[39mshow_save_dir:\n\u001b[1;32m   1948\u001b[0m   suffix \u001b[38;5;241m=\u001b[39m _filename_suffix_from_codec(codec)\n",
      "File \u001b[0;32m~/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/mediapy/__init__.py:1781\u001b[0m, in \u001b[0;36mcompress_video\u001b[0;34m(images, codec, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m directory_name:\n\u001b[1;32m   1780\u001b[0m   tmp_path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(directory_name) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuffix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1781\u001b[0m   \u001b[43mwrite_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1782\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tmp_path\u001b[38;5;241m.\u001b[39mread_bytes()\n",
      "File \u001b[0;32m~/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/mediapy/__init__.py:1753\u001b[0m, in \u001b[0;36mwrite_video\u001b[0;34m(path, images, **kwargs)\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m VideoWriter(path, shape\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mdtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m   1752\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m-> 1753\u001b[0m     \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/uha_test_policy/lib/python3.10/site-packages/mediapy/__init__.py:1646\u001b[0m, in \u001b[0;36mVideoWriter.add_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m   1644\u001b[0m     image \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdstack((image, image, image))\n\u001b[1;32m   1645\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (image\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m-> 1646\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage dimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are invalid.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape:\n\u001b[1;32m   1648\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1649\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage dimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not match\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1650\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m those of the initialized video \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1651\u001b[0m   )\n",
      "\u001b[0;31mValueError\u001b[0m: Image dimensions (1, 256, 256, 3) are invalid."
     ]
    }
   ],
   "source": [
    "# dataset = get_octo_dataset_tensorflow(cfg, train=True)\n",
    "dataset = get_single_dataset_tensorflow(cfg, train=True).repeat().unbatch()\n",
    "# dataset = get_sequential_dataset_tensorflow(cfg, train=True)\n",
    "is_single_dataset = True\n",
    "batch_size = 128\n",
    "# visual_dataset = (dataset.flatten().batch(batch_size))\n",
    "# create Pytorch Train Dataset\n",
    "dataloader = make_pytorch_oxe_iterable_dataset(dataset, train=True, batch_size=batch_size, transform_dict=cfg_transforms, num_workers=0, pin_memory=True, is_single_dataset=is_single_dataset, main_process=False)\n",
    "it = iter(dataloader)\n",
    "batch = next(it)\n",
    "# print(batch)\n",
    "# if batch[\"observation\"][\"image_primary\"].shape[-1] == 3:\n",
    "#   batch[\"observation\"][\"image_primary\"] = torch.moveaxis(batch[\"observation\"][\"image_primary\"], -1, -3) # move rgb to get [..., rgb, width, height]\n",
    "# if batch[\"observation\"][\"image_primary\"].dtype == torch.uint8:\n",
    "#   batch[\"observation\"][\"image_primary\"] = convert_image_dtype(batch[\"observation\"][\"image_primary\"], dtype=torch.float32)\n",
    "images, image_wrist = [], []\n",
    "for i in range(batch_size):\n",
    "  images.append(batch[\"observation\"][\"image_primary\"][i, 0].numpy()) # [batch_size, window_size, rgb, width, height]\n",
    "\n",
    "mediapy.show_video(images, fps=2)\n",
    "# mediapy.show_video(images_wrist, fps=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca123e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(episode['steps'].element_spec)\n",
    "print(steps[0]['language_instruction'])\n",
    "print(steps[0]['language_instruction_2'])\n",
    "print(steps[0]['language_instruction_3'])\n",
    "print(os.path.expanduser(\"~\") + \"/tensorflow_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_to_range(tensor, max_values, min_values) -> torch.Tensor:\n",
    "    # Scale the tensor to the new range [new_min, new_max]\n",
    "    new_min = -torch.ones_like(tensor)\n",
    "    new_max = torch.ones_like(tensor)\n",
    "    rescaled_tensor = (tensor - new_min) / (new_max - new_min) * (max_values - min_values) + min_values\n",
    "    return rescaled_tensor\n",
    "\n",
    "# run inference loop, this model only uses 3rd person image observations for bridge\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "for step in tqdm.trange(len(images)):\n",
    "    input_images = torch.from_numpy(np.moveaxis(images[step], 2, 0)).to(device).to(torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    input_images_wrist = torch.from_numpy(np.moveaxis(images_wrist[step], 2, 0)).to(device).to(torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    observation = {\n",
    "        'image_primary': input_images,\n",
    "        'image_wrist': input_images_wrist,\n",
    "        'timestep_pad_mask': np.full((1, input_images.shape[1]), True, dtype=bool)\n",
    "    }\n",
    "    goal = steps[step]['language_instruction'].numpy().decode(\"utf-8\")\n",
    "    tokenized_goal = tokenizer(goal, return_tensors = 'pt', padding = \"max_length\", truncation = True, max_length = 77)\n",
    "    tokenized_goal[\"input_ids\"] = tokenized_goal[\"input_ids\"].unsqueeze(0).to(device)\n",
    "    tokenized_goal[\"attention_mask\"] = tokenized_goal[\"attention_mask\"].unsqueeze(0).to(device)\n",
    "    task = {\"language_instruction\": tokenized_goal}\n",
    "    \n",
    "    # this returns *normalized* actions --> we need to unnormalize using the dataset statistics\n",
    "    actions = agent({'observation': observation, 'task': task})\n",
    "    train_stats = dataset_statistics[\"action\"]\n",
    "    # actions = rescale_to_range(actions[0], torch.Tensor(train_stats['p99']).to(device).to(torch.float32), torch.Tensor(train_stats['p01']).to(device).to(torch.float32)) # remove batch dim\n",
    "    actions = rescale_to_range(actions[0], torch.Tensor(train_stats['max']).to(device).to(torch.float32), torch.Tensor(train_stats['min']).to(device).to(torch.float32)) # remove batch dim\n",
    "\n",
    "    pred_actions.append(actions.cpu().numpy())\n",
    "    final_window_step = step\n",
    "    true_actions.append(steps[step]['action'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "## Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "print(pred_actions.shape)\n",
    "print(true_actions.shape)\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  # actions have batch, horizon, dim, in this example we just take the first action for simplicity\n",
    "  axs[action_label].plot(pred_actions[:, 0, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  # axs[action_label].plot(true_actions[:, action_dim] - pred_actions[:, 0, action_dim], label='error')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
